!pip install transformers>=4.40.1 accelerate>=0.27.2
#installing necessary transfomers so that the Transformers, language models and necesarry tokenizersa can be imported

from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    device_map = 'cuda',
    torch_dtype = 'auto',
    trust_remote_code = 'False'
)

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

#Creating a pipeline with parameters using the previously created pipeline and tokenizers
from transformers import pipeline

generator = pipeline(
transformer = transformer,
tokenizer = tokenizer.
return_full_text = False,
max_new_tokens = 500,
do_sample = False)

#creating a prompt as a user to see the output from the language model in order to see the output from Phi3

message = [ {'role':'user', "content": "craete a motivational post about moving back from USA to India"} ]
output = generator(message)
print(output[0]["generated_text"]
